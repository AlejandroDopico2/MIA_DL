{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aec18ab",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbcb3219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 20:53:54.989904: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-13 20:53:55.023162: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-13 20:53:55.023193: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-13 20:53:55.024275: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-13 20:53:55.030174: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-13 20:53:55.677649: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images : 202599\n",
      "Found 162770 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"/home/ana/Documents/dl-labs/P3/archive/\"\n",
    "\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "#from keras.utils import plot_model\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "filenames = np.array(glob.glob(os.path.join(DATA_FOLDER, '*/*.jpg')))\n",
    "NUM_IMAGES = len(filenames)\n",
    "print(\"Total number of images : \" + str(NUM_IMAGES))\n",
    "# prints : Total number of images : 100000\n",
    "\n",
    "INPUT_DIM = (128,128,3) # Image dimension\n",
    "BATCH_SIZE = 512\n",
    "Z_DIM = 200 # Dimension of the latent vector (z)\n",
    "\n",
    "\n",
    "data_flow = ImageDataGenerator(rescale=1./255).flow_from_directory(DATA_FOLDER, \n",
    "    target_size = INPUT_DIM[:2],\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    class_mode = 'input',\n",
    "    subset = 'training',\n",
    "    color_mode='rgb',\n",
    "    classes=['train']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf1a18",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ab646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)  [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " encoder_conv_0 (Conv2D)     (None, 64, 64, 16)           448       ['encoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 64, 64, 16)           0         ['encoder_conv_0[0][0]']      \n",
      "                                                                                                  \n",
      " encoder_conv_1 (Conv2D)     (None, 32, 32, 32)           4640      ['leaky_re_lu[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 32)           0         ['encoder_conv_1[0][0]']      \n",
      "                                                                                                  \n",
      " encoder_conv_2 (Conv2D)     (None, 16, 16, 32)           9248      ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 32)           0         ['encoder_conv_2[0][0]']      \n",
      "                                                                                                  \n",
      " encoder_conv_3 (Conv2D)     (None, 8, 8, 32)             9248      ['leaky_re_lu_2[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 8, 8, 32)             0         ['encoder_conv_3[0][0]']      \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 2048)                 0         ['leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " mu (Dense)                  (None, 200)                  409800    ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " log_var (Dense)             (None, 200)                  409800    ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " encoder_output (Lambda)     (None, 200)                  0         ['mu[0][0]',                  \n",
      "                                                                     'log_var[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 843184 (3.22 MB)\n",
      "Trainable params: 843184 (3.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.utils.vis_utils import plot_model\n",
    " \n",
    "# ENCODER\n",
    "def build_vae_encoder(input_dim, output_dim, conv_filters, conv_kernel_size, conv_strides):\n",
    "  \n",
    "    # Clear tensorflow session to reset layer index numbers to 0 for LeakyRelu, \n",
    "    # BatchNormalization and Dropout.\n",
    "    # Otherwise, the names of above mentioned layers in the model \n",
    "    # would be inconsistent\n",
    "    global K\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Number of Conv layers\n",
    "    n_layers = len(conv_filters)\n",
    "\n",
    "    # Define model input\n",
    "    encoder_input = Input(shape = input_dim, name = 'encoder_input')\n",
    "    x = encoder_input\n",
    "\n",
    "    # Add convolutional layers\n",
    "    for i in range(n_layers):\n",
    "        x = Conv2D(filters = conv_filters[i], \n",
    "            kernel_size = conv_kernel_size[i],\n",
    "            strides = conv_strides[i], \n",
    "            padding = 'same',\n",
    "            name = 'encoder_conv_' + str(i)\n",
    "            )(x)\n",
    "\n",
    "        x = LeakyReLU()(x)\n",
    "        \n",
    "    # Required for reshaping latent vector while building Decoder\n",
    "    shape_before_flattening = K.int_shape(x)[1:] \n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    mean_mu = Dense(output_dim, name = 'mu')(x)\n",
    "    log_var = Dense(output_dim, name = 'log_var')(x)\n",
    "\n",
    "    # Defining a function for sampling\n",
    "    def sampling(args):\n",
    "        mean_mu, log_var = args\n",
    "        epsilon = K.random_normal(shape=K.shape(mean_mu), mean=0., stddev=1.) \n",
    "        return mean_mu + K.exp(log_var/2)*epsilon   \n",
    "\n",
    "    # Using a Keras Lambda Layer to include the sampling function as a layer \n",
    "    # in the model\n",
    "    encoder_output = Lambda(sampling, name='encoder_output')([mean_mu, log_var])\n",
    "\n",
    "\n",
    "    return encoder_input, encoder_output, mean_mu, log_var, shape_before_flattening, Model(encoder_input, encoder_output)\n",
    "\n",
    "\n",
    "vae_encoder_input, vae_encoder_output,  mean_mu, log_var, vae_shape_before_flattening, vae_encoder  = build_vae_encoder(\n",
    "    input_dim = INPUT_DIM,\n",
    "    output_dim = Z_DIM, \n",
    "    #conv_filters = [32, 64, 64, 64],\n",
    "    conv_filters = [16, 32, 32, 32],\n",
    "    conv_kernel_size = [3,3,3,3],\n",
    "    conv_strides = [2,2,2,2])\n",
    "\n",
    "vae_encoder.summary()\n",
    "\n",
    "# plot_model(vae_encoder, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d97cc",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feab64d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 200)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              411648    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " decoder_conv_0 (Conv2DTran  (None, 16, 16, 32)        9248      \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " decoder_conv_1 (Conv2DTran  (None, 32, 32, 32)        9248      \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " decoder_conv_2 (Conv2DTran  (None, 64, 64, 16)        4624      \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " decoder_conv_3 (Conv2DTran  (None, 128, 128, 3)       435       \n",
      " spose)                                                          \n",
      "                                                                 \n",
      " activation (Activation)     (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 435203 (1.66 MB)\n",
      "Trainable params: 435203 (1.66 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_decoder(input_dim, shape_before_flattening, conv_filters, conv_kernel_size, conv_strides):\n",
    "\n",
    "    # Number of Conv layers\n",
    "    n_layers = len(conv_filters)\n",
    "\n",
    "    # Define model input\n",
    "    decoder_input = Input(shape = (input_dim,) , name = 'decoder_input')\n",
    "\n",
    "    # To get an exact mirror image of the encoder\n",
    "    x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "    x = Reshape(shape_before_flattening)(x)\n",
    "\n",
    "    # Add convolutional layers\n",
    "    for i in range(n_layers):\n",
    "        x = Conv2DTranspose(\n",
    "            filters = conv_filters[i], \n",
    "            kernel_size = conv_kernel_size[i],\n",
    "            strides = conv_strides[i], \n",
    "            padding = 'same',\n",
    "            name = 'decoder_conv_' + str(i)\n",
    "            )(x)\n",
    "        \n",
    "        # Adding a sigmoid layer at the end to restrict the outputs \n",
    "        # between 0 and 1\n",
    "        if i < n_layers - 1:\n",
    "            x = LeakyReLU()(x)\n",
    "        else:\n",
    "            x = Activation('sigmoid')(x)\n",
    "\n",
    "    # Define model output\n",
    "    decoder_output = x\n",
    "\n",
    "    return decoder_input, decoder_output, Model(decoder_input, decoder_output)\n",
    "\n",
    "decoder_input, decoder_output, vae_decoder = build_decoder(input_dim = Z_DIM,\n",
    "    shape_before_flattening = vae_shape_before_flattening,\n",
    "    #conv_filters = [64,64,32,3],        \n",
    "    conv_filters = [32,32,16,3],\n",
    "    conv_kernel_size = [3,3,3,3],\n",
    "    conv_strides = [2,2,2,2]\n",
    "    )\n",
    "\n",
    "vae_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702949c4",
   "metadata": {},
   "source": [
    "### Encoder + Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e944e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)  [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " encoder_conv_0 (Conv2D)     (None, 64, 64, 16)           448       ['encoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 64, 64, 16)           0         ['encoder_conv_0[0][0]']      \n",
      "                                                                                                  \n",
      " encoder_conv_1 (Conv2D)     (None, 32, 32, 32)           4640      ['leaky_re_lu[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 32)           0         ['encoder_conv_1[0][0]']      \n",
      "                                                                                                  \n",
      " encoder_conv_2 (Conv2D)     (None, 16, 16, 32)           9248      ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 32)           0         ['encoder_conv_2[0][0]']      \n",
      "                                                                                                  \n",
      " encoder_conv_3 (Conv2D)     (None, 8, 8, 32)             9248      ['leaky_re_lu_2[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 8, 8, 32)             0         ['encoder_conv_3[0][0]']      \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 2048)                 0         ['leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " mu (Dense)                  (None, 200)                  409800    ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " log_var (Dense)             (None, 200)                  409800    ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " encoder_output (Lambda)     (None, 200)                  0         ['mu[0][0]',                  \n",
      "                                                                     'log_var[0][0]']             \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, 128, 128, 3)          435203    ['encoder_output[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1278387 (4.88 MB)\n",
      "Trainable params: 1278387 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae_input = vae_encoder_input\n",
    "vae_output = vae_decoder(vae_encoder_output)\n",
    "\n",
    "# Input to the combined model will be the input to the encoder.\n",
    "# Output of the combined model will be the output of the decoder.\n",
    "vae = Model(vae_input, vae_output)\n",
    "\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5336a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reconstructing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a33b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_compare_VAE(images, add_noise=False):\n",
    "    \n",
    "    n_to_show = images.shape[0]\n",
    "\n",
    "    if add_noise:\n",
    "        encodings = VAE_encoder.predict(images)\n",
    "        encodings += np.random.normal(0.0, 1.0, size = (n_to_show,200))\n",
    "        reconst_images = VAE_decoder.predict(encodings)\n",
    "\n",
    "    else:\n",
    "        reconst_images = vae.predict(images)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    fig.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.95, hspace=0.1, wspace=0.1)\n",
    "\n",
    "    for i in range(n_to_show):\n",
    "        #img = images[i].squeeze()\n",
    "        img = images[i]\n",
    "        sub = fig.add_subplot(2, n_to_show, i+1)\n",
    "        sub.axis('off')        \n",
    "        sub.imshow(img,cmap='gray')\n",
    "\n",
    "    for i in range(n_to_show):\n",
    "        img = reconst_images[i].squeeze()\n",
    "        sub = fig.add_subplot(2, n_to_show, i+n_to_show+1)\n",
    "        sub.axis('off')\n",
    "        sub.imshow(img,cmap='gray')\n",
    "    plt.show()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada9f60",
   "metadata": {},
   "source": [
    "### Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03067dc5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 20:34:42.747491: W tensorflow/c/c_api.cc:305] Operation '{name:'count_2/Assign' id:1180 op device:{requested: '', assigned: ''} def:{{{node count_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_2, count_2/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-05-13 20:34:43.303557: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_1/mul' id:1260 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/model_1_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-05-13 20:34:43.362734: W tensorflow/c/c_api.cc:305] Operation '{name:'training_2/Adam/decoder_conv_2/kernel/m/Assign' id:1620 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/decoder_conv_2/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/decoder_conv_2/kernel/m, training_2/Adam/decoder_conv_2/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318/318 [==============================] - 92s 288ms/step - batch: 158.5000 - size: 511.8553 - loss: 261.0237 - r_loss: 0.0208 - kl_loss: 52.8788\n",
      "Epoch 2/2\n",
      " 10/318 [..............................] - ETA: 1:26 - batch: 4.5000 - size: 512.0000 - loss: 245.9937 - r_loss: 0.0192 - kl_loss: 54.2050"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# example_batch = next(data_flow)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# example_batch = example_batch[0]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# example_images = example_batch[:8]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_BLOCKS):\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_flow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# initial_epoch = 0, \u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# steps_per_epoch=NUM_IMAGES / BATCH_SIZE)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# plot_compare_VAE(example_images) \u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/dl-labs/P3/.venv/lib/python3.10/site-packages/keras/src/engine/training_v1.py:856\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    855\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 856\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dl-labs/P3/.venv/lib/python3.10/site-packages/keras/src/engine/training_generator_v1.py:647\u001b[0m, in \u001b[0;36mGeneratorOrSequenceTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    643\u001b[0m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps_per_epoch, x)\n\u001b[1;32m    644\u001b[0m training_utils_v1\u001b[38;5;241m.\u001b[39mcheck_generator_arguments(\n\u001b[1;32m    645\u001b[0m     y, sample_weight, validation_split\u001b[38;5;241m=\u001b[39mvalidation_split\n\u001b[1;32m    646\u001b[0m )\n\u001b[0;32m--> 647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dl-labs/P3/.venv/lib/python3.10/site-packages/keras/src/engine/training_generator_v1.py:239\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m step \u001b[38;5;241m<\u001b[39m target_steps:\n\u001b[0;32m--> 239\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m \u001b[43m_get_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_dataset:\n\u001b[1;32m    242\u001b[0m             \u001b[38;5;66;03m# The dataset passed by the user ran out of batches.  Now we\u001b[39;00m\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;66;03m# know the cardinality of the dataset.  If steps_per_epoch\u001b[39;00m\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;66;03m# was specified, then running out of data is unexpected, so\u001b[39;00m\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;66;03m# we stop training and inform the user.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/dl-labs/P3/.venv/lib/python3.10/site-packages/keras/src/engine/training_generator_v1.py:388\u001b[0m, in \u001b[0;36m_get_next_batch\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 388\u001b[0m     generator_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mStopIteration\u001b[39;00m, tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mOutOfRangeError):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/dl-labs/P3/.venv/lib/python3.10/site-packages/keras/src/utils/data_utils.py:862\u001b[0m, in \u001b[0;36mOrderedEnqueuer.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m    864\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mtask_done()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "N_EPOCHS = 2  # No. of epochs to show advance\n",
    "N_BLOCKS = 10\n",
    "LOSS_FACTOR = 10000\n",
    "\n",
    "\n",
    "adam_optimizer = Adam(learning_rate = LEARNING_RATE)\n",
    "\n",
    "def r_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "\n",
    "def kl_loss(y_true, y_pred):\n",
    "    kl_loss =  -0.5 * K.sum(1 + log_var - K.square(mean_mu) - K.exp(log_var), axis = 1)\n",
    "    return kl_loss\n",
    "\n",
    "def total_loss(y_true, y_pred):\n",
    "    return LOSS_FACTOR*r_loss(y_true, y_pred) + kl_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "vae.compile(optimizer=adam_optimizer, loss = total_loss, metrics = [r_loss, kl_loss])\n",
    "\n",
    "\n",
    "# example_batch = next(data_flow)\n",
    "# example_batch = example_batch[0]\n",
    "# example_images = example_batch[:8]\n",
    "\n",
    "for i in range(N_BLOCKS):\n",
    "    vae.fit(data_flow, \n",
    "        shuffle=True, \n",
    "        epochs = N_EPOCHS)\n",
    "        # initial_epoch = 0, \n",
    "        # steps_per_epoch=NUM_IMAGES / BATCH_SIZE)\n",
    "    # plot_compare_VAE(example_images) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa97769",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_PrefetchDataset' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_flow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: '_PrefetchDataset' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(data_flow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
